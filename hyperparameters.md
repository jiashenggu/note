当Batch Size增大时，学习率该如何随之变化？

SGD是线性缩放

在RMSProp,Adam,LAMB等优化器上，结果是平方根缩放

在大尺度batch size下，训练容易出现不稳定的情况，这个原因在于在训练过程中，gradient norm会出现大幅度的尖峰，这导致了参数和训练损失的不稳定（也即是尖峰），SigLip观察到，如果将Adam优化器的值从0.999下降到0.95，那么训练过程就会稳定下来。
