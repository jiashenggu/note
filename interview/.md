# Intro

Hi, it's a pleasure to meet you. I'm Jiasheng Gu.

At 01.AI, I was an engineer on a generative AI video product, building it from scratch. My main responsibility was the end-to-end training pipeline for our VLMs and LLMs, including 25-billion and 247-billion parameter MoE models.

My process had two key stages. First, I handled the continued pre-training on a 20-billion-token domain-specific dataset, using the Megatron-LM framework. Second, I designed and implemented our alignment pipeline using both SFT and DPO. This was crucial for teaching the model the complex nuances of video understanding and intelligent clipping.

While I've greatly valued the challenges and growth of my previous roles, after researching the potential of physical AI and being inspired by NVIDIA's vision, I came to believe that applying this technology to create physical AI is the essential next step, and this conviction was a key driver in my decision to seek new opportunities. The reason I'm so excited about this role is that I see a direct bridge between my work and the challenges of this role. My experience in teaching models to perceive and reason about the world through video can directly contribute to Project GR00T.

## Why DPO instead of PPO after SFT (≈ 30 seconds)

We chose DPO because it avoids the reward-hacking loop that often destabilizes PPO.

## Structured JSON output from the VLM (≈ 30 seconds)

To guarantee schema-valid JSON, we used constrained decoding with a context-free grammar compiled into a finite-state machine. At each token step we mask the logits so only legal continuations are possible. The overhead is about 1 millisecond, but the downstream parser never fails, which is critical for automatic video editing.

## Compensation expectation (≈ 20 seconds)

What matters most is the scope of the problems I can solve and the caliber of the team.

## Question: "What do you think of NVIDIA's approach with Isaac and Project GR00T?"

Your Potential Answer: "I've been following it closely, and I think NVIDIA's full-stack strategy is precisely what the field needs. It's incredibly ambitious. You're not just building a model; you're building the entire ecosystem required for success. This includes the specialized compute hardware like the Jetson Thor, Isaac Sim platform for scalable training, and finally, the GR00T foundation model that ties it all together. This vertical integration allows for optimizations at every level. The concept of GR00T as a 'generalist blueprint' that understands multimodal instructions and can be fine-tuned for different embodiments is the most promising path towards scalable robotics, and it aligns perfectly with my own experience in building and adapting large foundation models."

## Question: "Our strategy is to provide the platform and tools like Isaac and GR00T to enable the entire robotics ecosystem. How does this compare to more vertically integrated approaches, like Tesla's Optimus? What are the pros and cons in your view?"

"Tesla's vertical integration gives them tight control and the ability to optimize for a specific set of tasks within their own factories—a huge advantage for rapid, focused deployment. However, it's a closed garden. NVIDIA's platform strategy is fundamentally about enabling an entire market, much like Windows did for PCs or Android for mobile. The pro is massive scale and capturing a wider range of innovation from countless partners. The con is that it can be slower to get off the ground and requires managing a more complex ecosystem. I believe the platform approach will win in the long run because the diversity of real-world robotics applications is too vast for any single company to address alone."
# Questions Should Ask
## balance real data and simulated data
I would like to know how you handle the balance between real data and simulated data?


