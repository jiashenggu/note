sample-100BT: a subset randomly sampled from the whole dataset of around 100B gpt2 tokens (277.4GB)

sample-10BT: a subset randomly sampled from the whole dataset of around 10B gpt2 tokens (27.6GB)

Benchmark measurements for MPT models trained on MosaicML platform, including throughput, MFU, and HFU.

https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/benchmarking/README.md

Approximately **2 hours** to train 1 billion tokens on a 34-billion-parameter model using 64 H100 GPUs.

Approximately **8 hours** to train 1 billion tokens on a 34-billion-parameter model using 16 H100 GPUs.
